---
title: "range_report"
author: "odenas"
date: "August 14, 2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
```

Range queries take as input a pair of positions in the MS
vector of the type $[a, b)$ (with $0 \leq a \leq b \leq |query|$)
and return a function of MS values in those positions. We consider
here the `sum` and `max` of MS values in a range.

# Sum Range queries on vanilla compression 
```{r}
rm(list=ls())

rel_diff_f <- function(a, b) (a - b) / pmax(a, b)
base_dir <- "./vanilla_compression_techniques/sum"

read_time_dataset <- function(input_id){
  time_dd <- (
    read_csv(sprintf("%s/%s.t.ms.csv", base_dir, input_id))
  )  %>% mutate(time_ms = time_ms / nqueries, dset=input_id)
  time_dd
}
```
The trivial approach is to keep a partial sum as the values of MS that fall in the input
interval are encountered . This approach needs to issue 2 select queries in `ms`
and a linear time scan of the corresponding bit vector portion. 

One can speedup scanning of the `ms` portion by parsing whole words of 64 bits.
We call this the djamal algorithm in the plots. We implemented this also 
for various compression methods in SDSL and RLE based compression. The figure below
shows the performance of these implementations as a function of range size $b - a$.
```{r}
dd <- (
  rbind(
    read_time_dataset("mm")
  )
  %>% filter(method == 'algorithm', block_size==0)
  %>% select(-method)
  %>% pivot_wider(names_from = algo, values_from = time_ms)
  %>% mutate(speedup = trivial / djamal)
)

ggplot(dd, aes(range_size, speedup, color=compression)) +
  geom_line() + geom_point() + 
  expand_limits(y=c(0, 1)) +
  scale_x_log10() + scale_y_log10() +
  labs(subtitle = "block_size = 0", y="trivial / djamal")
```

If multiple range queries are expected, using an index data structure
that stores partial sum values at fixed size blocks (block spanning
strings of `0`s are counted in the next block). The algorithm then can
simply add the block sums in the set of blocks properly contained
in the interval and naive scanning in the blocks spanning the interval
endpoints.

The figure below shows clear speedups from the use of an index.

```{r}
dd <- (
  rbind(
    read_time_dataset("mm")
  )
  %>% filter(compression=='none', algo=='trivial', method == 'algorithm')
  %>% select(-nqueries, -compression, -method, -algo)
  %>% pivot_wider(names_from = block_size, values_from = time_ms, names_prefix="b_")
  %>% transmute(range_size=range_size, `4` = b_0 / b_4, `1e+06` = b_0 / `b_1e+06`)
  %>% pivot_longer(cols = c(`4`, `1e+06`), names_to = "block_size", values_to = "speedup")
)

ggplot(dd, aes(range_size, speedup, color=factor(block_size))) +
  geom_line() + geom_point() +
  scale_y_log10() + scale_x_log10() +
  labs(subtitle = "index speedup on trivial algorithm", y="no-index / index")
```

We also applied the djamal algorithm on indexed queries. but it didn't do as on uncompressed data.

```{r}
dd <- (
  rbind(
    read_time_dataset("mm")
  )
  %>% filter(method == "algorithm", dset == "mm")
  %>% select(-nqueries, -method, -dset)
  %>% pivot_wider(names_from = algo, values_from = time_ms)
  %>% mutate(speedup = trivial / djamal)
)

ggplot(dd, aes(range_size, speedup, color=factor(block_size))) +
  geom_line() +
  scale_x_log10() + scale_y_log10() +
  facet_wrap(~compression) +
  labs(subtitle = "djamal method speedup", y="trivial / djamal")
```

# Max Range queries on vanilla compression 
```{r}
rm(list=ls())

rel_diff_f <- function(a, b) (a - b) / pmax(a, b)
base_dir <- "./vanilla_compression_techniques/max"
algorithm_dot <- c("algorithm.select", "algorithm.blocks", 
                   "algorithm.trivial_case",  "algorithm.trivial_scan", 
                   "algorithm.rmq_query", "algorithm.rmq_scan")
range_dot <- c("range.bit", "range.block", "range.i_block", "range.int")

read_time_dataset <- function(input_id){
  time_dd <- (
    read_csv(sprintf("%s/%s.t.ms.csv", base_dir, input_id))
    %>% filter(method %in% c('algorithm', algorithm_dot, range_dot))
    %>% mutate(time_ms = time_ms / nqueries, dset=input_id)
  )
  time_dd
}
```
The trivial approach is to scan the values of MS that fall in the input
interval and keep a current max. This approach needs 2 select queries in `ms`
and a linear time scan of the corresponding bit vector portion. 

One can speedup scanning of the `ms` portion by parsing whole words of 64 bits.
We call this the djamal algorithm in the plots. We implemented this also 
for various compression methods in SDSL and RLE based compression. The figure below
shows the performance of these implementations as a function of range size $b - a$.

```{r}
dd <- (rbind(read_time_dataset('mm'))
       %>% filter(method == 'algorithm', block_size == 0, compression != "rrr")  # djamal in rrr not supported
       %>% select(-method, -block_size)
       %>% pivot_wider(names_from = algo, values_from = time_ms)
       %>% mutate(speed_up = trivial / djamal)
       )

ggplot(dd, aes(range_size, speed_up, color=compression)) +
  geom_line() + geom_point() +
  expand_limits(y=c(0, 1)) +
  scale_x_log10() +
  labs(subtitle = "djamal speedup on trivial algorithm", y="trivial / djamal")
```

If multiple range queries are expected, using an index data structure
that stores partial max values at fixed size blocks (block spanning
strings of `0`s are counted in the next block). The algorithm uses
RMQ to find the max value in the set of blocks properly contained
in the interval and naive scanning in the blocks spanning the interval
endpoints.

The figure below shows the time performance as a function of range size and
block size. The vertical lines are placed at range size positions equal to the
corresponding block size. The labels of the vertical lines indicate the size
of the input range in the bit vector. Notice

* block size = 0 refers to the trivial algorithm with no index
* range sizes up to 2 block sizes match the no-index algorithm,
  larger ranges will contain entire blocks inside and make use of RMQ,
  hence the performance remains constant thereafter.
  
```{r}
dd <- (
  rbind( read_time_dataset("mm") )
  %>% filter(compression=='none', algo == 'trivial')
  %>% select(-nqueries, -algo, -compression)
)

size_dd <- (
  dd 
  %>% filter(method=='range.bit') %>% select(dset, method, block_size, range_size, time_ms) %>% rename(brsize = time_ms)
  %>% group_by(dset, block_size) %>% mutate(d = rank(abs(block_size - range_size)))
  %>% ungroup() %>% filter(d == 1) %>% select(-d)
)
time_dd <- (dd %>% filter(method == 'algorithm'))
avg_dd <- time_dd %>% group_by(block_size) %>% summarise(aa = mean(time_ms)) %>% filter(block_size > 0)

ggplot(time_dd, aes(range_size, time_ms, group=factor(block_size))) +
  geom_line(aes(color=factor(block_size))) +  geom_point(aes(color=factor(block_size))) + 
  geom_vline(data = size_dd, aes(xintercept=brsize, color=factor(block_size)), linetype="dashed", alpha=1) +
  #geom_text(data = size_dd %>% mutate(a=max(time_dd$time_ms) * 2 + runif(nrow(size_dd))/2),
  #          aes(brsize, a, label=format(round(brsize, 0), big.mark = ",", scientific = FALSE), angle=90), size=3) +
  #geom_hline(data = avg_dd, aes(yintercept=aa, color=factor(block_size)), linetype="dashed", alpha=1) +
  #geom_text(data = avg_dd, aes(min(time_dd$range_size) / 2, aa, label=format(round(aa, 0), big.mark = ",", scientific = FALSE)), size=3) +
  #expand_limits(y=c(0, 1)) +
  scale_y_log10() + scale_x_log10() +
  #facet_wrap(~block_size, scales="free_y") +
  labs(subtitle = "trivial algorithm, uncompressed")
```

# Space
`ms` compression sizes

```{r}
base_dir <- "./vanilla_compression_techniques"
read_sizes_dataset <- function(input_id){
  get_size <- function(e){
    file.info(sprintf('%s/%s.t.ms.%s', base_dir, input_id, e))$size
  }
  space_dd <- (
    tibble(compression = c('delta', 'nibble', 'none', 'rle', 'rrr', 'succint'))
    %>% mutate(size = sapply(compression, get_size))
  )
  
  space_dd <- rbind(space_dd, space_dd %>% filter(compression == 'none') %>% mutate(compression = 'word'))

  space_dd <- (
    space_dd %>% mutate(gr = 1)
    %>% inner_join(
      space_dd %>% filter(compression == "none") %>% transmute(gr = 1, nsize=size),
      by=c("gr"))
    %>% mutate(size_diff = rel_diff_f(size, nsize))
    %>% select(-gr)
  )
  space_dd  
}

space_dd <- rbind(read_sizes_dataset('HG03061-2') %>% mutate(dset='human_human'),
                  read_sizes_dataset('mm') %>% mutate(dset='chimp_human'))
space_dd
```

