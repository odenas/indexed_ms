---
title: "Report of experiments"
author: "O. Denas"
date: "12/28/2016"
output:
  pdf_document: 
    number_sections: yes
    toc: yes
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
\newpage
# Input properties

For various types of inputs ("mut_XMs_YMt_Z" means `s` and `t` are random identical strings of length X, and Y million respectively with mutations inserted every Z characters. "rnd_XMs_YMt" means `s` and `t` are random strings of length X, and Y million respectively)  run the MS algorithm and count the number of 

 * consecutive `parent()` calls during the `runs` construction.
 * consecutive `wl()` calls during the `ms` construction. 
 * the number of 1s in the `runs` bit vector
 * double rank calls that fail (i.e the search down the WT is interrupted prior to reaching a leaf)
 * the number of maximal repeats


```{r, echo=FALSE, message=FALSE, fig.height=4, fig.width=5}
rm(list = ls())
source('utils.R')
```

## Consecutive parent calls (RUNS construction)
```{r, echo=FALSE, message=FALSE, fig.height=6, fig.width=4}
st_data <- read_ds("../input_stats_data/stats.csv", FALSE) %>% separate(b_path, into=c("stype", "sl", "ttype", "tl", "alp")) %>% unite(inp_type, stype, ttype)

parent_data <- (st_data %>% filter(measuring == "consecutive_parent_calls", where=="runs"))
p <- (ggplot(parent_data %>% filter(inp_type != "rep")) + 
        theme(axis.text.x = element_text(angle = 90, hjust = 1, size=5), 
              strip.text.x = element_text(size = 7)) + 
        labs(subtitle = "Counting consecutive parent() calls"))

p + geom_bar(mapping = aes(x = factor(as.numeric(key)), y = value), stat = "identity") +
  facet_grid(inp_type~alp, scales = "free")
```

The input with repeats has a very different distribution from above.

```{r, echo=FALSE, message=FALSE, fig.height=4, fig.width=5, eval=FALSE}
pc_ds <- (parent_data %>% filter(inp_type == "rep") %>% 
            mutate(consecutive_parent_length = as.numeric(key), cnt = value) %>% 
            select(consecutive_parent_length, cnt))

ggplot(pc_ds, aes(consecutive_parent_length, cnt)) + 
  geom_point() + geom_smooth(se=FALSE) + scale_y_log10() + 
  labs(subtitle = "Consecutive parent call counts for input with repeats", x = "Lenght of consecutive parent call sequence", y = "Log10(count)")
rm(pc_ds, parent_data, p)
```

## Consecutive wl calls (MS construction)
```{r, echo=FALSE, message=FALSE, fig.height=4, fig.width=5}
wl_data <- (st_data %>% filter(measuring == "consecutive_wl_calls", where=="ms"))

p <- (ggplot(wl_data %>% filter(inp_type != "rep")) + 
        theme(axis.text.x = element_text(angle = 90, hjust = 1, size=5), 
              strip.text.x = element_text(size = 7)) + 
        labs(subtitle = "Counting consecutive wl() calls"))

p + geom_bar(mapping = aes(x = factor(as.numeric(key)), y = value), stat = "identity") +
  facet_grid(inp_type~alp, scales = "free")
```


```{r, echo=FALSE, message=FALSE, fig.height=4, fig.width=5, eval=FALSE}
wl_ds <- (wl_data %>% filter(inp_type == "rep") %>% 
            mutate(consecutive_wl_length = as.numeric(key), cnt = value) %>% 
            select(consecutive_wl_length, cnt))

ggplot(wl_ds, aes(consecutive_wl_length, cnt)) + 
  geom_point() + geom_smooth(se=FALSE) + scale_y_log10() + 
  labs(subtitle = "Consecutive parent call counts for input with repeats", x = "Lenght of consecutive parent call sequence", y = "Log10(count)")
rm(p, wl_data, wl_ds)
```

\newpage

## Other stats
```{r, echo=FALSE, message=FALSE, fig.height=4, fig.width=5}
mrep_data <- (st_data 
              %>% filter(measuring == "wlnode_prop", where == "ms") 
              %>% select(inp_type, key, value) 
              %>% separate(key, into=c('char', 'maximality', 'wl_presence', 'iwidth')) 
              %>% unite(key, maximality, wl_presence, iwidth) 
              %>% group_by(inp_type, key) %>% summarise(value = sum(value)))


aa <- (left_join(mrep_data, mrep_data %>% group_by(inp_type) %>% summarise(tot = sum(value)), by="inp_type")
      %>% mutate(perc = round(100 * value / tot, 0))
      %>% select(inp_type, key, perc) 
      %>% spread(key = inp_type, value = perc) 
      %>% separate(key, into=c('maximality', 'wl_presence', 'iwidth'))
)
ggplot(gather(aa, rep_dis, rep_sim, rnd_dis, rnd_sim, key='itp', value=cnt) 
       %>% group_by(maximality, wl_presence, itp) 
       %>% summarise(cnt = sum(cnt, na.rm = TRUE))
       %>% unite(ntp, maximality, wl_presence),
       aes(ntp, cnt, fill=itp, color=itp)) + 
  geom_line(aes(group=itp)) + geom_point() 

kable(left_join(mrep_data, mrep_data %>% group_by(inp_type) %>% summarise(tot = sum(value)), by="inp_type")
      %>% mutate(perc = round(100 * value / tot, 2))
      %>% select(inp_type, key, perc) 
      %>% spread(key = inp_type, value = perc) 
      %>% separate(key, into=c('maximality', 'wl_presence', 'iwidth')),
      caption="Distribution of node types (in percentage) when wl() calls are made.")

rm(aa, st_data, mrep_data)
```

\newpage
# Current performance
```{r, echo=FALSE, message=FALSE}
ds <- read_ds("../current_performance/res.csv") %>% separate(label, into=c("lazy", "fail", "maxrep"))

kable(filter(ds, item == "total_time") %>% select(lazy, fail, maxrep, value) %>% 
        mutate(total_s = value / 1000) %>%
        select(lazy, fail, maxrep, total_s) %>% 
        arrange(total_s), 
      caption=sprintf("Run time in seconds"))
rm(ds)
```

\newpage
# Parent sequence vs. lca

TODO:


\newpage
# Double vs. single rank
## Rank support optimization
The optimization occurs first at `rank_support_v.hpp` where we avoid recomputing a major block for intervals that are going to fall on the same major block anyways. 

The condition that checks whether endpoints $(i, j)$ of an interval end up in the same major block is 

```
bool((i>>8) == (j>>8))
```

### Code
The single rank and double rank implementations in sdsl: rank_support_v.hpp [link](https://github.com/odenas/indexed_ms/blob/master/sdsl-lite/include/sdsl/rank_support_v.hpp)

```
// RANK(idx)
const uint64_t* p = m_basic_block.data() + ((idx>>8)&0xFFFFFFFFFFFFFFFEULL);
return *p + ((*(p+1)>>(63 - 9*((idx&0x1FF)>>6)))&0x1FF) +
      (idx&0x3F ? trait_type::word_rank(m_v->data(), idx) : 0);

// DOUBLE RANK OD(i, j)
if((i>>8) == (j>>8)){
  const uint64_t* p = m_basic_block.data() + ((i>>8)&0xFFFFFFFFFFFFFFFEULL);
  res.first  = *p + ((*(p+1)>>(63 - 9*((i&0x1FF)>>6)))&0x1FF) + 
            (i&0x3F ? trait_type::word_rank(m_v->data(), i) : 0); 
  res.second = *p + ((*(p+1)>>(63 - 9*((j&0x1FF)>>6)))&0x1FF) + 
            (j&0x3F ? trait_type::word_rank(m_v->data(), j) : 0); 
} else {
  const uint64_t* p = m_basic_block.data() + ((i>>8)&0xFFFFFFFFFFFFFFFEULL);
  res.first  = *p + ((*(p+1)>>(63 - 9*((i&0x1FF)>>6)))&0x1FF) + 
            (i&0x3F ? trait_type::word_rank(m_v->data(), i) : 0); 
  p -= (((i>>8)&0xFFFFFFFFFFFFFFFEULL) - ((j>>8)&0xFFFFFFFFFFFFFFFEULL));
  res.second = *p + ((*(p+1)>>(63 - 9*((j&0x1FF)>>6)))&0x1FF) + 
            (j&0x3F ? trait_type::word_rank(m_v->data(), j) : 0); 
}
return res

// DOUBLE RANK FC(i, j)
const uint64_t* b = m_basic_block.data();
const uint64_t* pi = b + ((i>>8)&0xFFFFFFFFFFFFFFFEULL);
const uint64_t* pj = b + ((j>>8)&0xFFFFFFFFFFFFFFFEULL);

return (*pi + ((*(pi+1)>>(63 - 9*((i&0x1FF)>>6)))&0x1FF) + 
              (i&0x3F ? trait_type::word_rank(m_v->data(), i) : 0),
      *pj + ((*(pj+1)>>(63 - 9*((j&0x1FF)>>6)))&0x1FF) + 
              (j&0x3F ? trait_type::word_rank(m_v->data(), j) : 0));
```

\newpage
### Performance
The FC implementation seems to work better and will be adopted from now on.
```{r, echo=FALSE, message=FALSE, fig.width=4, fig.height=4}
od <- read_ds('../single_rank_vs_double_rank/rank_timing.csv.od') %>% filter(item != "dstruct") %>% mutate(label='OD')
fc <- read_ds('../single_rank_vs_double_rank/rank_timing.csv.fc') %>% filter(item != "dstruct") %>% mutate(label='FC')

dd <- rbind(fc, od) %>% group_by(item, label) %>% summarise(avg_time = mean(time_ms), sd_time = sd(time_ms))
kable(dd, digits=2, caption="Time (in ms) of 500K calls to `wl()` based on `single_rank()` or `double_rank()` methods on 100MB random DNA input; Mean/sd over 20 repetitions.")

performance_boxplot(ggplot(rbind(fc, od), aes(item, time_ms, color = label)), "performance", "time (ms)")
```

\newpage
## Weiner Link optimization -- single vs. double vs. double&fail rank
###Sandbox performance

TODO: describe dataset and tests

```{r, echo=FALSE, message=FALSE}
ds <- (read_ds('../single_rank_vs_double_rank/sandbox/rank_timing.csv') %>% 
         mutate(wl_presence = ifelse(has_wl, "has_wl", "no_wl"), 
                interval_width = ifelse(close, "same_block", "different_block")) %>%
         select(method, wl_presence, interval_width, time_ms, char, ntrial) %>% filter(char != 't'))

kable(spread(ds %>% group_by(method, interval_width, wl_presence) %>% summarise(avg_time = mean(time_ms)), key = method, value = avg_time),
      digits = 2, caption = "Sandbox performance of the two tricks")


performance_boxplot(ggplot(ds, aes(interval_width, time_ms, color=method)), 
                    "Sandbox tests", "ms") + facet_grid(~wl_presence)
```

\newpage

### Full algorithm performance

```{r, echo=FALSE, message=FALSE}
dds <- read_ds('../single_rank_vs_double_rank/rank_timing.csv', FALSE) %>%  filter(measuring == "time", item == "ms_bvector") %>% mutate(time_ms = 100000 * value / len_t) 
dds$b_path <- factor(dds$b_path, levels = (sorted_bpaths(dds$b_path) %>% arrange(inp_type, mu_nr))$b_path)

performance_boxplot(ggplot(dds, aes(b_path, time_ms, color=label)), 
                    "Time to build the ms vector", "ms") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + coord_flip()

kable(spread(dds %>% select(b_path, label, time_ms) %>% group_by(b_path, label) %>% summarize(avg_time = mean(time_ms)), key = label, value = avg_time))
```

\newpage

# Maxrep
## Maxrep construction
```{r, echo=FALSE}
optimized <- c(1020, 1023, 999, 1020, 1015)
existing <- c(1098, 1116, 1120, 1094, 1100)
```

Applying the first optimization (avoid visiting subtrees of non-maximal nodes) we get `r round(100 * (mean(existing) - mean(optimized)) / mean(existing), 0)`% improvement on a (ran of a 1MB input string).

##Sandbox performance

TODO: describe dataset and tests

```{r, echo=FALSE, message=FALSE, fig.height=8}
ds <- (read_ds('../use_maxrepeat/sandbox/rank_timing.csv') %>% 
         mutate(wl_presence = ifelse(has_wl, "has_wl", "no_wl"), 
                i_width = ifelse(close, "same_block", "different_block"),
                maximality = ifelse(is_maximal, "maxrep", "nomaxrep")) %>%
         select(method, wl_presence, i_width, time_ms, char, maximality))

kable(spread(ds %>% group_by(method, i_width, wl_presence, maximality) %>% 
               summarise(avg_time = mean(time_ms)), 
             key = i_width, value = avg_time) %>% 
        arrange(wl_presence, maximality) %>% 
        select(wl_presence, maximality, method, different_block, same_block),
      digits = 2, caption = "Sandbox performance of the two tricks")

performance_boxplot(ggplot(ds, aes(i_width, time_ms, color=method)), 
                    "Sandbox tests", "ms") +
  facet_grid(wl_presence~maximality, scales = "free_y")
```

\newpage

## Full Algorithm Performance
```{r, echo=FALSE, message=FALSE}
ds <- (read_ds('../use_maxrepeat/res.csv', FALSE) %>% 
         filter(measuring == "time", item == "ms_bvector") %>% 
         mutate(time_ms = value / len_t) # time / symbol in t
       )
ds$b_path <- factor(ds$b_path, 
                    levels = (sorted_bpaths(ds$b_path) %>% 
                                arrange(inp_type, mu_nr))$b_path)
ntrials <- 8
```
The figure below shows `r ntrials` runs of the program with and without the use of the `maxrep` (or `B`) vector. The plot shows times (in seconds) for the construction of the `ms` bitvector. The table below that, shows the time (in seconds) to construct the `maxrep` vector. The input data is random and has |s|=`r sprintf("%dMB", ds$len_s[1] / 10^6)` and |t|=`r sprintf("%dMB", ds$len_t[1] / 10^6)`.

```{r, echo=FALSE, message=FALSE}
performance_boxplot(ggplot(ds, aes(b_path, time_ms, color=label)), 
                    "Time to build the ms vector", "ms")
rm(ds, ntrials)
```

Check

```{r, echo=FALSE, message=FALSE}
library(stringr)
node_prop <- function(w, m){
  sapply(1:length(w), function(i){
    ans <- c(if(w[i] == "same_block") "narrow" else "wide",
             if(m[i] == "maxrep") "max" else "nonmax")
    str_c(ans, collapse = "_")
  })
}

sandbox_ds <- (read_ds('../use_maxrepeat/sandbox/rank_timing.csv') %>% 
                 filter(method %in% c("double_rank_fail", "double_rank_maxrep")) %>% 
                 mutate(wl_presence = ifelse(has_wl, "wl", "nowl"), 
                        iwidth = ifelse(close, "narrow", "wide"),
                        maximality = ifelse(is_maximal, "maxrep", "nomaxrep")) %>%
                 filter(char != "#") %>% 
                 group_by(method, char, maximality, wl_presence, iwidth) %>% 
                 summarise(time_ms = mean(time_ms))
               )
sandbox_ds

comp_ds <- (read_ds("../input_stats_data/rep_10Ms_1Mt_abcd.csv", TRUE) %>% 
              filter(measuring == "wlnode_prop", inp_type == "rep") %>% 
              select(key, value) %>% 
              separate(key, into=c("char", "maximality", "wl_presence", "iwidth")) %>% 
              mutate(count = value))
comp_ds

(inner_join(sandbox_ds, comp_ds, on = c("char", "maximality", "wl_presence", "iwidth")) %>% 
    mutate(total_time = (count * time_ms) / 100000) %>% 
    group_by(method) %>% summarise(total_time = sum(total_time)))

inner_join(sandbox_ds, comp_ds, on = c("char", "maximality", "wl_presence", "iwidth")) %>% 
  mutate(total_time = (count * time_ms) / 100000) %>% 
  select(method, char, maximality, wl_presence, iwidth, total_time) %>% 
  spread(method, total_time)


perf_ds <- (read_ds('../use_maxrepeat/res.csv') %>% 
              filter(measuring == "time", item == "ms_bvector") %>% 
              group_by(label) %>% 
              summarise(value_avg = mean(value), value_sd = sd(value)) %>% 
              select(label, value_avg, value_sd)
            )
perf_ds
```



\newpage

# Lazy vs non-lazy
## Code
The lazy and non-lazy versions differ in a couple of lines of code as follows

```
if(flags.lazy){
    for(; I.first <= I.second && h_star < ms_size; ){
        c = t[h_star];
        I = bstep_interval(st, I, c); //I.bstep(c);
        if(I.first <= I.second){
            v = st.lazy_wl(v, c); 
            h_star++;
        }   
    }   
    if(h_star > h_star_prev) // // we must have called lazy_wl(). complete the node
        st.lazy_wl_followup(v);
} else { // non-lazy weiner links
    for(; I.first <= I.second && h_star < ms_size; ){
        c = t[h_star];
        I = bstep_interval(st, I, c); //I.bstep(c);
        if(I.first <= I.second){
            v = st.wl(v, c); 
            h_star++;
        }   
    }   
}
```
\newpage
## Performance
```{r, echo=FALSE, warning=FALSE, message=FALSE}
dds <- (read_ds('../lazy_vs_nonlazy_data/lazy_vs_nonlazy.csv', FALSE) %>% filter(measuring == "time", item == 'ms_bvector') %>% select(len_s, len_t, value, item, label, b_path))
dds$b_path <- factor(dds$b_path, levels = (sorted_bpaths(dds$b_path) %>% arrange(inp_type, mu_nr))$b_path)

performance_boxplot(ggplot(dds, aes(b_path, value/1000, color=label)), 
                    "Time to build the ms vector", "seconds") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
rm(dds)
```

The right panel shows the time to construct the `runs` vector. This stage is the same for both versions and is shown as a control. On the left panel it can be seen that speedup correlates positively with both the size of the indexed string and the mutation period.

\newpage
## Sandbox timing
Measure the time of 10k repetitions of 

 * (lazy) $n$ consecutive `lazy_wl()` calls followed by a `lazy_wl_followup()`
 * (nonlazy) $n$ consecutive `wl()` calls
 * (lazy_nf) $n$ consecutive `lazy_wl()` calls

```
// lazy
for(size_type i = 0; i < trial_length; i++)
    v = st.lazy_wl(v, s_rev[k--]);
if(h_star > h_star_prev) // // we must have called lazy_wl(). complete the node
    st.lazy_wl_followup(v);
...
// non-lazy
for(size_type i = 0; i < trial_length; i++)
    v = st.wl(v, s_rev[k--]);
...
// lazy_nf
for(size_type i = 0; i < trial_length; i++)
    v = st.lazy_wl(v, s_rev[k--]);
```


```{r, echo=FALSE, warning=FALSE, message=FALSE}
sandbox_ds <- read_ds('../lazy_vs_nonlazy_data/sandbox_timing.csv') %>% filter(item != "dstruct", s==1e9, nwlcalls<100)

fit_lazy <- lm(time_ms ~ nwlcalls, data=(sandbox_ds %>% filter(item=="lazy")))
fit_nonlazy <- lm(time_ms ~ nwlcalls, data=(sandbox_ds %>% filter(item=="nonlazy")))
fit_lazy_nf <- lm(time_ms ~ nwlcalls, data=(sandbox_ds %>% filter(item=="lazy_nf")))

ggplot(sandbox_ds, aes(nwlcalls, time_ms,color=item)) + geom_point() + 
  geom_abline(intercept = coef(fit_lazy)[1], slope = coef(fit_lazy)[2], color='red', alpha=0.5) + 
  geom_abline(intercept = coef(fit_nonlazy)[1], slope = coef(fit_nonlazy)[2], color='blue', alpha=0.5) +
  geom_abline(intercept = coef(fit_lazy_nf)[1], slope = coef(fit_lazy_nf)[2], color='green', alpha=0.5) +
  labs(title = "indexed input size 1G", 
       subtitle=sprintf("lazy: %.2f + %.4f*n; nonlazy: %.2f + %.4f*n; lazy_nf: %.2f + %.4f*n", 
                        coef(fit_lazy)[1], coef(fit_lazy)[2],
                        coef(fit_nonlazy)[1], coef(fit_nonlazy)[2],
                        coef(fit_lazy_nf)[1], coef(fit_lazy_nf)[2]), 
       x="trial_length",  y=sprintf("time of 10K reps (ms)")) + 
  expand_limits(x=0, y=0) #+ scale_x_continuous(breaks=0:500)

ggplot(read_ds('../lazy_vs_nonlazy_data/sandbox_timing.csv') %>% filter(item != "dstruct", nwlcalls < 100),
       aes(as.factor(nwlcalls), time_ms, fill=item)) + 
  geom_bar(stat='identity', position='dodge') + 
  facet_wrap(~s) +
  labs(title = "absolute times for s=100M and s=1G") + 
  xlab("trial_length") + ylab("time (ms)") +
  theme(legend.position=c(.15, .8))

rm(sandbox_ds, fit_lazy, fit_nonlazy, fit_lazy_nf)
```

## Check
In the experiments above we ran the program with the "lazy" or "non-lazy" flag and measured. The total time of each experiment can be written as $t_l = l_l + a$ and  $t_n = l_n + a$ for the two versions respectively; only the $t$s being known. Furthermore, we have  $\hat{l}_l$ and $\hat{l}_n$ estimations -- computed by combining the time / wl call with the number of with the count of wl calls in each input (Section "Input Properties"). Hence we should expect 

$$
\delta t = t_l - t_n = l_l + a - l_n - a = l_l - l_n \approx \delta\hat{l} = \hat{l}_l - \hat{l}_n
$$


```{r, echo=FALSE, warning=FALSE, message=FALSE}
read_lazy_call_cnt <- function(path = '../lazy_vs_nonlazy_data/lazy.csv'){
  lazy_calls <- sapply(0:3000, function(i) sprintf("consecutive_lazy_wl_calls%d", i))
  ds <- (read_ds(path) %>%
           filter(item %in% lazy_calls, label=="lazy") %>%
           mutate(count = value) %>%
           group_by(b_path, item) %>%
           summarise(len_s = mean(len_s),
                     len_t = mean(len_t),
                     value = mean(value),
                     count = mean(count)))
  ds$nwlcalls = as.numeric(sapply(ds$item, function(s) substr(s, 26, 300)))
  ds
}

sandbox_timing_path <- '../lazy_vs_nonlazy_data/sandbox_timing.csv'
fit_lazy_g <- lm(time_ms ~ nwlcalls, read_ds(sandbox_timing_path) %>% 
                   filter(item != "dstruct", s==1e9, item=="lazy"))
fit_nonlazy_g <- lm(time_ms ~ nwlcalls, read_ds(sandbox_timing_path) %>% 
                      filter(item != "dstruct", s==1e9, item!="lazy"))
fit_lazy_m <- lm(time_ms ~ nwlcalls, read_ds(sandbox_timing_path) %>% 
                   filter(item != "dstruct", s==1e8, item=="lazy"))
fit_nonlazy_m <- lm(time_ms ~ nwlcalls, read_ds(sandbox_timing_path) %>% 
                      filter(item != "dstruct", s==1e8, item!="lazy"))

wl_time_ds <- rbind(filter(read_lazy_call_cnt(), len_s == 1e9) %>% group_by(b_path) %>% 
                      summarise(lazy = (count %*% predict(fit_lazy_g, data.frame(nwlcalls = nwlcalls)) / 10000)[1,1] / 1000,
                                nonlazy = (count %*% predict(fit_nonlazy_g, data.frame(nwlcalls = nwlcalls)) / 10000)[1, 1] / 1000), 
                    filter(read_lazy_call_cnt(), len_s == 1e8) %>% group_by(b_path) %>% 
                      summarise(lazy = (count %*% predict(fit_lazy_m, data.frame(nwlcalls = nwlcalls)) / 10000)[1,1] / 1000,
                                nonlazy = (count %*% predict(fit_nonlazy_m, data.frame(nwlcalls = nwlcalls)) / 10000)[1, 1] / 1000))


kable(merge(wl_time_ds %>% 
        mutate(delta_l_hat = round(lazy - nonlazy, 2), 
               l_l=round(lazy, 2), l_n=round(nonlazy/1, 2)) %>% 
        select(b_path, delta_l_hat, l_l, l_n),
      spread(read_ds('../lazy_vs_nonlazy_data/lazy_vs_nonlazy.csv') %>% 
              filter(measuring == "time", item == "ms_bvector") %>% 
              select(b_path, label, value) %>%
              group_by(b_path, label) %>% 
              summarise(value=mean(value)), key = label, value = value) %>%
        mutate(delta_t = round((lazy - nonlazy) / 1000, 2),
               t_l = round(lazy/1000, 2), t_n = round(nonlazy/1000, 2)) %>% 
        select(b_path, t_l, t_n, delta_t)) %>% select(b_path, t_l, t_n, l_l, l_n, delta_t, delta_l_hat), digits=2, row.names=FALSE)




ccc <-  round(with(merge(wl_time_ds %>% 
                           mutate(pred_abs_diff_sec = round(lazy - nonlazy, 2)) %>% 
                           select(b_path, pred_abs_diff_sec),
                         spread(read_ds('../lazy_vs_nonlazy_data/lazy_vs_nonlazy.csv') %>% 
                                  filter(measuring == "time", item == "ms_bvector") %>% 
                                  select(b_path, label, value) %>%
                                  group_by(b_path, label) %>% 
                                  summarise(value=mean(value)), 
                                key = label, value = value) %>%
                           mutate(actual_abs_diff_sec = round((lazy - nonlazy) / 1000, 2)) %>% 
                           select(b_path, actual_abs_diff_sec)), 
                   cor(pred_abs_diff_sec, actual_abs_diff_sec)), 2)
rm(wl_time_ds, fit_nonlazy_m, fit_lazy_m, fit_nonlazy_g, fit_lazy_g, sandbox_timing_path)
```


The numbers are not identical (process dependent factors might influence the running time of function calls), but they are correlated ($corr(\delta t, \delta\hat{l})$ = `r ccc`).
```{r, echo=FALSE, warning=FALSE, message=FALSE}
rm(ccc)
```

\newpage
# Double rank and fail
## Code
```
// Given subtree_double_rank(v, i, j) -> (a.first, a.second) -- to simplify code

// DOUBLE RANK: int i, int j, char c
p = bit_path(c)
result_i, result_j = i, j;
node_type v = m_tree.root();
for (l = 0; l < path_len; ++l, p >>= 1) {
  a = subtree_double_rank(v, m_tree.bv_pos(v) + result_i, m_tree.bv_pos(v) + result_j);

  if(p&1){ // left child
      if(result_i > 0) result_i = a.first; 
      if(result_j > 0) result_j = a.second; 
  } else { // right child
      if(result_i > 0) result_i -= a.first;
      if(result_j > 0) result_j -= a.second;
  }   
  v = m_tree.child(v, p&1); // goto child
}   
return(result_i, result_j)

// DOUBLE RANK AND FAIL
p = bit_path(c)
result_i, result_j = i, j;
node_type v = m_tree.root();
for (l = 0; l < path_len; ++l, p >>= 1) {
  a = subtree_double_rank(v, m_tree.bv_pos(v) + result_i, m_tree.bv_pos(v) + result_j);

  if(p&1){ // left child
      if(result_i > 0) result_i = a.first; 
      if(result_j > 0) result_j = a.second; 
  } else { // right child
      if(result_i > 0) result_i -= a.first;
      if(result_j > 0) result_j -= a.second;
  }   
  if(result_i == result_j) // Weiner Link call will fail
    return(0, 0)
  v = m_tree.child(v, p&1); // goto child
}
return(result_i, result_j)
```
\newpage
## Performance
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=3, fig.width=4}
#ddd <- rbind(read_ds('../double_rank_and_fail/double_rank_and_fail.csv') %>% mutate(label="fail"),
#      read_ds('../double_rank_and_fail/double_rank.csv') %>% mutate(label="no_fail")) %>% #mutate(time_ms = value) %>% filter(measuring == "time", item %in% c("ms_bvector", "runs_bvector"))

ddd <- read_ds('../double_rank_and_fail/res.csv') %>% mutate(time_ms = value) %>% filter(measuring == "time", item %in% c("ms_bvector", "runs_bvector"))

dd <- group_by(ddd, item, label, b_path) %>% 
  summarise(avg_time = mean(time_ms), sd_time = sd(time_ms))
kable(dd, digits=2, caption="Time (in ms) of 500K calls to `wl()` based on `single_rank()` or `double_rank()` methods on 100MB random DNA input; Mean/sd over 20 repetitions.")


rank_comp <- spread(dd %>% select(item, label, avg_time), key = label, value = avg_time) %>% mutate(abs_ratio = double_rank_and_fail / double_rank, rel_ratio = 100 * abs(double_rank_and_fail - double_rank) / double_rank)
kable(data.frame(rank_comp),
      digits=2, caption="Single vs. double rank. Absolute (double / single) and relative (100 * |double - single| / single) ratios of average times.", row.names=FALSE)

ggplot(ddd, aes(item, time_ms, color=label)) + geom_boxplot() + geom_jitter(width=0.1, alpha=0.3) + ylab('total time (ms)') + facet_wrap(~b_path)
rm(dd, ddd, rank_comp)
```


\newpage
# Parallelization
## Code 
See the pseudo-code in the repo ([link](https://github.com/odenas/indexed_ms/blob/master/notes/reupdate/parallel_MS.pdf))

## Performance
Run the MS construction program on the same input (random strings $s$ of length 100M and $t$ of length 5M) with varying parallelzation degree (nthreads = number of threads).

The time is reported over 5 runs for each fixed number of threads.

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=5}
read_all <- function() do.call(rbind, lapply(list.files('../parallelization_data/', 'results.*.threads.csv'), function(n) read_ds(sprintf('../parallelization_data/%s', n), FALSE)))

aa <- read_all() %>% filter(measuring == "time")
#aa$b_path <- simplify2array(lapply(strsplit(aa$b_path, "_", fixed = TRUE), parse_b_path))

ggplot(aa, aes(factor(nthreads), value/1000, color=b_path)) + geom_boxplot() + facet_wrap(~item, scales = "free_y", nrow = 3) + ylab('seconds') + labs(title = "Time usage")
rm(aa)
```

Space in MB for the same settings as above. 

Each thread allocates its own $ms$ vector with initial size $|t| / nthreads$ then it resizes by a factor of 1.5 each time it needs to. Resizing will always result in a vector smaller than $2|t|$ elements.

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=3, fig.width=4}
alloc_str <- sapply(0:30, function(i) sprintf("ms_bvector_allocated%d", i))
used_str <- sapply(0:30, function(i) sprintf("ms_bvector_used%d", i))

aa <- (read_all() %>% 
         filter(measuring == "space", item %in% alloc_str) %>% 
         group_by(len_s, len_t, measuring, label, b_path, nthreads) %>% 
         summarise(value = mean(value), item="ms_bvector_allocated") %>% 
         mutate(value = value * nthreads))

bb <- (read_all() %>% 
         filter(measuring == "space", item %in% used_str) %>%
         group_by(len_s, len_t, measuring, label, b_path, nthreads, item) %>% 
         summarise(value = mean(value)) %>% 
         group_by(len_s, len_t, measuring, label, b_path, nthreads) %>% 
         summarise(value = sum(value), item="ms_bvector_used"))


ggplot(rbind(aa, bb), aes(factor(nthreads), value / 1000000, fill=item, group=item, color=item)) + geom_bar(stat='identity', position='dodge')  + ylab('MB') + labs(title = "Space usage")
rm(aa, alloc_str, bb, read_all, used_str)
```

